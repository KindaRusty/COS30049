{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **TechNova - Spam Detection (AI Model Training)**\n",
        "\n",
        "## **COS30049 - Computing Technology Innovation Project**\n",
        "\n",
        "---\n",
        "\n",
        "### **Project Overview**\n",
        "This notebook presents a comprehensive Machine Learning pipeline for **Spam Detection** - classifying text messages/emails as either **Spam** or **Ham** (not spam).\n",
        "\n",
        "The pipeline includes:\n",
        "1. **Data Loading & Exploration** - Understanding the dataset distribution and characteristics\n",
        "2. **Data Preprocessing** - Text cleaning, normalization, and feature engineering\n",
        "3. **Feature Extraction** - TF-IDF Vectorization\n",
        "4. **Model Training** - Naive Bayes, SVM, Logistic Regression, Random Forest\n",
        "5. **Model Evaluation** - Accuracy, Precision, Recall, F1-Score, Confusion Matrix, ROC Curve\n",
        "6. **Cross-Validation** - 5-Fold CV for robust performance estimation\n",
        "7. **Hyperparameter Tuning** - GridSearchCV for optimal model configuration\n",
        "8. **Model Saving** - Exporting the best model and vectorizer with joblib\n",
        "9. **Prediction Interface** - Function to classify new text inputs\n"
      ],
      "metadata": {
        "id": "fmjEia21-J3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 1: Setup & Import Libraries**\n",
        "\n",
        "We use the following libraries:\n",
        "- **pandas** - data manipulation and analysis\n",
        "- **numpy** - numerical operations\n",
        "- **matplotlib & seaborn** - data visualization\n",
        "- **scikit-learn** - ML models, preprocessing, evaluation\n",
        "- **nltk** - natural language processing (stopwords, tokenization)\n",
        "- **wordcloud** - word cloud visualization\n",
        "- **joblib** - model serialization\n"
      ],
      "metadata": {
        "id": "5ZRijEJR-J3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "# pip install pandas numpy matplotlib seaborn scikit-learn nltk wordcloud joblib"
      ],
      "metadata": {
        "id": "gPzkNc2S-J3f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_curve, auc\n",
        ")\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Visualization\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Model saving\n",
        "import joblib\n",
        "\n",
        "# Set plot style\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "plt.rcParams[\"font.size\"] = 12\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ],
      "metadata": {
        "id": "EEKFqC2j-J3g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Upload & Load Dataset**\n",
        "\n",
        "Upload your CSV dataset file. The dataset should contain text messages/emails with their corresponding labels (spam/ham).\n"
      ],
      "metadata": {
        "id": "wYQziGft-J3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Google Colab: Upload files\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your dataset file(s) (.csv):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "filenames = list(uploaded.keys())\n",
        "print(f\"\\nUploaded files: {filenames}\")\n"
      ],
      "metadata": {
        "id": "CzcMevSN-J3h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and merge data\n",
        "dfs = []\n",
        "print(\"--- Loading Data ---\")\n",
        "\n",
        "for file in filenames:\n",
        "    try:\n",
        "        # Try reading with 'utf-8' first, then 'latin1' if it fails\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"  'utf-8' decoding failed for {file}. Trying 'latin1'...\")\n",
        "            df = pd.read_csv(file, encoding='latin1')\n",
        "\n",
        "        dfs.append(df)\n",
        "        print(f\"Loaded {file}: {df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "if dfs:\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"\\nTotal shape after merging: {df.shape}\")\n",
        "else:\n",
        "    raise ValueError(\"No data loaded!\")\n"
      ],
      "metadata": {
        "id": "noxb5Ade-J3h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 2: Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Before building any model, we need to understand our dataset thoroughly.\n"
      ],
      "metadata": {
        "id": "rZ1VcvCw-J3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Basic Data Information**\n"
      ],
      "metadata": {
        "id": "5ld9ugxF-J3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows\n",
        "print(\"=\" * 60)\n",
        "print(\"FIRST 5 ROWS\")\n",
        "print(\"=\" * 60)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "idH-J8Ax-J3i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset info\n",
        "print(\"=\" * 60)\n",
        "print(\"DATASET INFO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nData Types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nDuplicate Rows: {df.duplicated().sum()}\")\n"
      ],
      "metadata": {
        "id": "Cj06rMTB-J3i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical summary\n",
        "print(\"=\" * 60)\n",
        "print(\"STATISTICAL SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "df.describe(include=\"all\")\n"
      ],
      "metadata": {
        "id": "k6AwEE9x-J3i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Target Distribution**\n"
      ],
      "metadata": {
        "id": "Q93TGsEn-J3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the label column\n",
        "# Adjust this if your dataset uses a different column name\n",
        "label_col = \"v1\"\n",
        "text_col = \"v2\"\n",
        "\n",
        "print(f\"Label column: \\\"{label_col}\\\"\")\n",
        "print(f\"Text column: \\\"{text_col}\\\"\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(df[label_col].value_counts())\n",
        "print(f\"\\nClass proportions:\")\n",
        "print(df[label_col].value_counts(normalize=True).round(4) * 100)"
      ],
      "metadata": {
        "id": "HcZn2yhP-J3i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize class distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart\n",
        "colors = [\"#2ecc71\", \"#e74c3c\"]\n",
        "df[label_col].value_counts().plot(kind=\"bar\", ax=axes[0], color=colors, edgecolor=\"black\")\n",
        "axes[0].set_title(\"Class Distribution (Bar Chart)\", fontsize=14, fontweight=\"bold\")\n",
        "axes[0].set_xlabel(\"Class\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "axes[0].tick_params(axis=\"x\", rotation=0)\n",
        "\n",
        "# Pie chart\n",
        "df[label_col].value_counts().plot(kind=\"pie\", ax=axes[1], autopct=\"%1.1f%%\",\n",
        "                                   colors=colors, startangle=90,\n",
        "                                   textprops={\"fontsize\": 12})\n",
        "axes[1].set_title(\"Class Distribution (Pie Chart)\", fontsize=14, fontweight=\"bold\")\n",
        "axes[1].set_ylabel(\"\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YzRKLj-9-J3j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 Text Length Analysis**\n"
      ],
      "metadata": {
        "id": "e1VbQwRk-J3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add text length features\n",
        "df[\"text_length\"] = df[text_col].apply(len)\n",
        "df[\"word_count\"] = df[text_col].apply(lambda x: len(str(x).split()))\n",
        "df[\"special_char_count\"] = df[text_col].apply(lambda x: sum(1 for c in str(x) if c in string.punctuation))\n",
        "\n",
        "print(\"Text Length Statistics by Class:\")\n",
        "print(df.groupby(label_col)[[\"text_length\", \"word_count\", \"special_char_count\"]].describe().round(2))\n"
      ],
      "metadata": {
        "id": "EKft9jgJ-J3j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of text lengths by class\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "features = [\"text_length\", \"word_count\", \"special_char_count\"]\n",
        "titles = [\"Text Length Distribution\", \"Word Count Distribution\", \"Special Character Count\"]\n",
        "\n",
        "for i, (feat, title) in enumerate(zip(features, titles)):\n",
        "    for label in df[label_col].unique():\n",
        "        subset = df[df[label_col] == label]\n",
        "        axes[i].hist(subset[feat], bins=50, alpha=0.6, label=label, edgecolor=\"black\")\n",
        "    axes[i].set_title(title, fontsize=13, fontweight=\"bold\")\n",
        "    axes[i].set_xlabel(feat)\n",
        "    axes[i].set_ylabel(\"Frequency\")\n",
        "    axes[i].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-_-qPZaP-J3j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.4 Word Cloud Visualization**\n"
      ],
      "metadata": {
        "id": "eqqJMoCH-J3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Cloud for Spam vs Ham\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "for i, label in enumerate(df[label_col].unique()):\n",
        "    text = \" \".join(df[df[label_col] == label][text_col].astype(str).tolist())\n",
        "    wc = WordCloud(width=800, height=400, background_color=\"white\",\n",
        "                   max_words=200, colormap=\"viridis\").generate(text)\n",
        "    axes[i].imshow(wc, interpolation=\"bilinear\")\n",
        "    axes[i].set_title(f\"Word Cloud \\u2014 {label.upper()}\", fontsize=14, fontweight=\"bold\")\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VNppPVBL-J3j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 3: Data Preprocessing**\n",
        "\n",
        "Preprocessing is crucial for NLP tasks. We will:\n",
        "1. Remove duplicates and handle missing values\n",
        "2. Clean text: lowercase, remove punctuation, remove stopwords\n",
        "3. Encode labels (spam \\u2192 1, ham \\u2192 0)\n",
        "4. Split data into train/test sets\n"
      ],
      "metadata": {
        "id": "EccFboc8-J3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Data Cleaning**\n"
      ],
      "metadata": {
        "id": "qy0RFUTC-J3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates and missing values\n",
        "print(f\"Shape before cleaning: {df.shape}\")\n",
        "print(f\"Duplicates: {df.duplicated().sum()}\")\n",
        "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.dropna(subset=[text_col, label_col], inplace=True)\n",
        "\n",
        "print(f\"\\nShape after cleaning: {df.shape}\")\n"
      ],
      "metadata": {
        "id": "TXS506s9-J3j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Text Preprocessing**\n"
      ],
      "metadata": {
        "id": "f1by_fxo-J3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning function\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text:\n",
        "    1. Convert to lowercase\n",
        "    2. Remove URLs\n",
        "    3. Remove email addresses\n",
        "    4. Remove numbers\n",
        "    5. Remove punctuation\n",
        "    6. Remove stopwords\n",
        "    7. Remove extra whitespace\n",
        "    \"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)                   # Remove emails\n",
        "    text = re.sub(r\"\\d+\", \"\", text)                        # Remove numbers\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words and len(w) > 1]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply cleaning\n",
        "print(\"Cleaning text data...\")\n",
        "df[\"cleaned_text\"] = df[text_col].apply(clean_text)\n",
        "\n",
        "# Show before vs after\n",
        "print(\"\\n--- Before vs After Cleaning ---\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nOriginal:  {df[text_col].iloc[i][:100]}...\")\n",
        "    print(f\"Cleaned:   {df['cleaned_text'].iloc[i][:100]}...\")\n",
        "print(\"\\nText cleaning complete!\")\n"
      ],
      "metadata": {
        "id": "U9Li2VLY-J3j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3 Label Encoding**\n"
      ],
      "metadata": {
        "id": "xpd-XxhH-J3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels: ham \\u2192 0, spam \\u2192 1\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[label_col])\n",
        "\n",
        "print(\"Label Encoding Mapping:\")\n",
        "for cls, encoded in zip(le.classes_, le.transform(le.classes_)):\n",
        "    print(f\"  {cls} \\u2192 {encoded}\")\n",
        "\n",
        "print(f\"\\nEncoded distribution:\")\n",
        "print(df[\"label\"].value_counts())\n"
      ],
      "metadata": {
        "id": "dw_GI76V-J3j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.4 Feature Extraction \\u2014 TF-IDF Vectorization**\n",
        "\n",
        "TF-IDF (Term Frequency \\u2014 Inverse Document Frequency) converts text into numerical feature vectors.\n",
        "- **TF**: How often a word appears in a document\n",
        "- **IDF**: Penalizes words that appear in many documents (common words)\n"
      ],
      "metadata": {
        "id": "FxRxJWCc-J3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "print(\"--- TF-IDF Vectorization ---\")\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words=\"english\",\n",
        "                        ngram_range=(1, 2),   # unigrams + bigrams\n",
        "                        min_df=2, max_df=0.95)\n",
        "\n",
        "X = tfidf.fit_transform(df[\"cleaned_text\"])\n",
        "y = df[\"label\"]\n",
        "\n",
        "print(f\"TF-IDF Matrix shape: {X.shape}\")\n",
        "print(f\"Number of features (vocabulary size): {len(tfidf.get_feature_names_out())}\")\n",
        "print(f\"\\nSample feature names: {list(tfidf.get_feature_names_out()[:20])}\")\n"
      ],
      "metadata": {
        "id": "nI7PwupH-J3k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.5 Train/Test Split**\n"
      ],
      "metadata": {
        "id": "-3JX-qib-J3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data: 80% train, 20% test (stratified to maintain class proportions)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set:  {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nTesting class distribution:\")\n",
        "print(y_test.value_counts())\n"
      ],
      "metadata": {
        "id": "NSQlNu_N-J3k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 4: Model Training & Evaluation**\n",
        "\n",
        "We will train and compare **4 different classification algorithms**:\n",
        "1. **Multinomial Naive Bayes** \\u2014 Fast, works well with text data\n",
        "2. **Support Vector Machine (SVM)** \\u2014 Effective in high-dimensional spaces\n",
        "3. **Logistic Regression** \\u2014 Simple yet powerful linear classifier\n",
        "4. **Random Forest** \\u2014 Ensemble method, robust to overfitting\n"
      ],
      "metadata": {
        "id": "zxAuRRLx-J3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1 Define Models**\n"
      ],
      "metadata": {
        "id": "QI9_DtYR-J3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "models = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"SVM (LinearSVC)\": CalibratedClassifierCV(LinearSVC(random_state=42, max_iter=10000)),\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "print(f\"Models to train: {list(models.keys())}\")\n"
      ],
      "metadata": {
        "id": "FOOgf02X-J3k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2 Train & Evaluate All Models**\n"
      ],
      "metadata": {
        "id": "8DppO98h-J3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate each model\n",
        "results = {}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL TRAINING & EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'\\u2500' * 50}\")\n",
        "    print(f\"Training: {name}\")\n",
        "    print(f\"{'\\u2500' * 50}\")\n",
        "\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, average=\"binary\")\n",
        "    rec = recall_score(y_test, y_pred, average=\"binary\")\n",
        "    f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
        "\n",
        "    results[name] = {\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1-Score\": f1,\n",
        "        \"model\": model,\n",
        "        \"y_pred\": y_pred\n",
        "    }\n",
        "\n",
        "    print(f\"  Accuracy:  {acc:.4f}\")\n",
        "    print(f\"  Precision: {prec:.4f}\")\n",
        "    print(f\"  Recall:    {rec:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "    print(f\"\\n  Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "print(\"\\nAll models trained successfully!\")\n"
      ],
      "metadata": {
        "id": "AnRXSFC0-J3k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 5: Model Comparison & Visualization**\n"
      ],
      "metadata": {
        "id": "LWNQSdB0-J3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.1 Confusion Matrices**\n"
      ],
      "metadata": {
        "id": "4HmfeSSz-J3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrices for all models\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "for idx, (name, res) in enumerate(results.items()):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    cm = confusion_matrix(y_test, res[\"y_pred\"])\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
        "                xticklabels=le.classes_, yticklabels=le.classes_,\n",
        "                annot_kws={\"size\": 14})\n",
        "    ax.set_title(f\"Confusion Matrix \\u2014 {name}\", fontsize=13, fontweight=\"bold\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nG6tpcVO-J3k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.2 Performance Comparison**\n"
      ],
      "metadata": {
        "id": "uBqOVhIU-J3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    name: {k: v for k, v in res.items() if k not in [\"model\", \"y_pred\"]}\n",
        "    for name, res in results.items()\n",
        "}).T\n",
        "\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(comparison_df.round(4).to_string())\n",
        "\n",
        "# Bar chart comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "comparison_df.plot(kind=\"bar\", ax=ax, colormap=\"Set2\", edgecolor=\"black\", width=0.8)\n",
        "ax.set_title(\"Model Performance Comparison\", fontsize=15, fontweight=\"bold\")\n",
        "ax.set_ylabel(\"Score\")\n",
        "ax.set_xlabel(\"Model\")\n",
        "ax.set_ylim(0.7, 1.02)\n",
        "ax.legend(loc=\"lower right\", fontsize=10)\n",
        "ax.tick_params(axis=\"x\", rotation=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ERPtSK3K-J3u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.3 ROC Curve**\n"
      ],
      "metadata": {
        "id": "j6HCJ7nf-J3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Curves\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#f39c12\"]\n",
        "\n",
        "for idx, (name, res) in enumerate(results.items()):\n",
        "    model = res[\"model\"]\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        y_prob = model.decision_function(X_test)\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    ax.plot(fpr, tpr, color=colors[idx], lw=2,\n",
        "            label=f\"{name} (AUC = {roc_auc:.4f})\")\n",
        "\n",
        "ax.plot([0, 1], [0, 1], \"k--\", lw=1, label=\"Random Classifier\")\n",
        "ax.set_title(\"ROC Curve Comparison\", fontsize=15, fontweight=\"bold\")\n",
        "ax.set_xlabel(\"False Positive Rate\", fontsize=12)\n",
        "ax.set_ylabel(\"True Positive Rate\", fontsize=12)\n",
        "ax.legend(loc=\"lower right\", fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4bTqhQrV-J3u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 6: Cross-Validation (5-Fold)**\n",
        "\n",
        "Cross-validation provides a more robust estimate of model performance by training and testing on different subsets of data.\n"
      ],
      "metadata": {
        "id": "NtpJbZx4-J3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-Fold Stratified Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_results = {}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"5-FOLD CROSS-VALIDATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Re-define models (fresh instances)\n",
        "cv_models = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"SVM (LinearSVC)\": CalibratedClassifierCV(LinearSVC(random_state=42, max_iter=10000)),\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "for name, model in cv_models.items():\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring=\"f1\")\n",
        "    cv_results[name] = {\n",
        "        \"Mean F1\": scores.mean(),\n",
        "        \"Std F1\": scores.std(),\n",
        "        \"Scores\": scores\n",
        "    }\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  F1 Scores per fold: {[f'{s:.4f}' for s in scores]}\")\n",
        "    print(f\"  Mean F1: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
      ],
      "metadata": {
        "id": "_DiXqMkr-J3u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize cross-validation results\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "names = list(cv_results.keys())\n",
        "means = [cv_results[n][\"Mean F1\"] for n in names]\n",
        "stds = [cv_results[n][\"Std F1\"] for n in names]\n",
        "\n",
        "bars = ax.bar(names, means, yerr=stds, capsize=5,\n",
        "              color=[\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\"],\n",
        "              edgecolor=\"black\")\n",
        "ax.set_title(\"5-Fold Cross-Validation \\u2014 Mean F1-Score\", fontsize=14, fontweight=\"bold\")\n",
        "ax.set_ylabel(\"F1-Score\")\n",
        "ax.set_ylim(0.7, 1.02)\n",
        "ax.tick_params(axis=\"x\", rotation=10)\n",
        "\n",
        "for bar, mean in zip(bars, means):\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
        "            f\"{mean:.4f}\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FWayA-wp-J3u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 7: Hyperparameter Tuning (GridSearchCV)**\n",
        "\n",
        "We use **GridSearchCV** to find the optimal hyperparameters for our best-performing model.\n"
      ],
      "metadata": {
        "id": "sEXw9k44-J3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine best model from CV results\n",
        "best_model_name = max(cv_results, key=lambda n: cv_results[n][\"Mean F1\"])\n",
        "print(f\"Best model from CV: {best_model_name}\")\n",
        "print(f\"Mean F1: {cv_results[best_model_name]['Mean F1']:.4f}\")\n"
      ],
      "metadata": {
        "id": "l4uuyoUX-J3u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    \"Naive Bayes\": {\n",
        "        \"alpha\": [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "    },\n",
        "    \"SVM (LinearSVC)\": {\n",
        "        \"estimator__C\": [0.01, 0.1, 1.0, 10.0],\n",
        "        \"estimator__max_iter\": [5000, 10000]\n",
        "    },\n",
        "    \"Logistic Regression\": {\n",
        "        \"C\": [0.01, 0.1, 1.0, 10.0, 100.0],\n",
        "        \"solver\": [\"lbfgs\", \"liblinear\"]\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        \"n_estimators\": [50, 100, 200],\n",
        "        \"max_depth\": [None, 10, 20, 30],\n",
        "        \"min_samples_split\": [2, 5]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Fresh instance of the best model\n",
        "best_model_instances = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"SVM (LinearSVC)\": CalibratedClassifierCV(LinearSVC(random_state=42)),\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "print(f\"\\n--- GridSearchCV for {best_model_name} ---\")\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=best_model_instances[best_model_name],\n",
        "    param_grid=param_grids[best_model_name],\n",
        "    cv=5,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV F1-Score: {grid_search.best_score_:.4f}\")\n"
      ],
      "metadata": {
        "id": "a-FLj43b-J3u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate tuned model on test set\n",
        "tuned_model = grid_search.best_estimator_\n",
        "y_pred_tuned = tuned_model.predict(X_test)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"TUNED {best_model_name.upper()} \\u2014 TEST SET RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_tuned):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_tuned):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred_tuned):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y_test, y_pred_tuned):.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_tuned, target_names=le.classes_))\n",
        "\n",
        "# Compare before vs after tuning\n",
        "orig_f1 = results[best_model_name][\"F1-Score\"]\n",
        "tuned_f1 = f1_score(y_test, y_pred_tuned)\n",
        "print(f\"Before tuning F1: {orig_f1:.4f}\")\n",
        "print(f\"After tuning F1:  {tuned_f1:.4f}\")\n",
        "print(f\"Improvement:      {(tuned_f1 - orig_f1):.4f}\")\n"
      ],
      "metadata": {
        "id": "jJHM5-G6-J3v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 8: Save Best Model**\n",
        "\n",
        "We save the best model and the TF-IDF vectorizer using **joblib** so they can be loaded later for the web application.\n"
      ],
      "metadata": {
        "id": "zPDyNx_9-J3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and vectorizer\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "model_path = \"saved_models/spam_model.pkl\"\n",
        "vectorizer_path = \"saved_models/tfidf_vectorizer.pkl\"\n",
        "encoder_path = \"saved_models/label_encoder.pkl\"\n",
        "\n",
        "joblib.dump(tuned_model, model_path)\n",
        "joblib.dump(tfidf, vectorizer_path)\n",
        "joblib.dump(le, encoder_path)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(f\"Vectorizer saved to: {vectorizer_path}\")\n",
        "print(f\"Label encoder saved to: {encoder_path}\")\n",
        "\n",
        "# Verify file sizes\n",
        "for path in [model_path, vectorizer_path, encoder_path]:\n",
        "    size_kb = os.path.getsize(path) / 1024\n",
        "    print(f\"   {path}: {size_kb:.1f} KB\")\n"
      ],
      "metadata": {
        "id": "ebutCzgT-J3v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 9: Prediction Interface**\n",
        "\n",
        "Create a reusable function to predict whether a given text is spam or ham.\n"
      ],
      "metadata": {
        "id": "3F2MPaeQ-J3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved model (simulating deployment)\n",
        "loaded_model = joblib.load(model_path)\n",
        "loaded_vectorizer = joblib.load(vectorizer_path)\n",
        "loaded_encoder = joblib.load(encoder_path)\n",
        "\n",
        "def predict_spam(text, model=loaded_model, vectorizer=loaded_vectorizer, encoder=loaded_encoder):\n",
        "    \"\"\"\n",
        "    Predict whether a text message is Spam or Ham.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text message\n",
        "        model: Trained ML model\n",
        "        vectorizer: Fitted TF-IDF vectorizer\n",
        "        encoder: Fitted label encoder\n",
        "\n",
        "    Returns:\n",
        "        dict: Prediction result with label and confidence\n",
        "    \"\"\"\n",
        "    # Clean the text\n",
        "    cleaned = clean_text(text)\n",
        "\n",
        "    # Vectorize\n",
        "    features = vectorizer.transform([cleaned])\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(features)[0]\n",
        "    label = encoder.inverse_transform([prediction])[0]\n",
        "\n",
        "    # Confidence score\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        proba = model.predict_proba(features)[0]\n",
        "        confidence = max(proba)\n",
        "    else:\n",
        "        confidence = None\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"prediction\": label,\n",
        "        \"confidence\": f\"{confidence:.2%}\" if confidence else \"N/A\"\n",
        "    }\n",
        "\n",
        "print(\"Prediction function ready!\")\n"
      ],
      "metadata": {
        "id": "gQDD_hIc-J3v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Test predictions with sample texts\n",
        "sample_texts = [\n",
        "    \"Congratulations! You have won a $1000 Walmart gift card. Click here to claim now!\",\n",
        "    \"Hey, are you free for lunch tomorrow? Let me know.\",\n",
        "    \"URGENT: Your account has been compromised. Verify your identity immediately at this link.\",\n",
        "    \"Hi Mom, I will be home for dinner tonight. See you at 7!\",\n",
        "    \"FREE entry in 2 weekly competitions. Text WIN to 80808. Conditions apply.\",\n",
        "    \"Can you send me the notes from today's lecture? Thanks!\",\n",
        "    \"You have been selected for a secret shopper position. Earn $500/day working from home!\",\n",
        "    \"Meeting rescheduled to 3 PM. Please confirm your attendance.\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SPAM DETECTION \\u2014 SAMPLE PREDICTIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for text in sample_texts:\n",
        "    result = predict_spam(text)\n",
        "    icon = \"SPAM\" if result[\"prediction\"] == \"spam\" else \"HAM\"\n",
        "    display_text = text[:80] + (\"...\" if len(text) > 80 else \"\")\n",
        "    print(f\"\\n[{icon}] (Confidence: {result['confidence']})\")\n",
        "    print(f\"   \\\"{display_text}\\\"\")\n"
      ],
      "metadata": {
        "id": "vIWVavZg-J3v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Step 10: Summary & Conclusions**\n",
        "\n",
        "### **Key Findings:**\n",
        "\n",
        "1. **Dataset**: We analyzed a dataset of text messages labeled as spam or ham.\n",
        "2. **Preprocessing**: Text was cleaned using NLTK (lowercase, remove stopwords, punctuation, URLs, numbers).\n",
        "3. **Feature Extraction**: TF-IDF vectorization with unigrams and bigrams (max 5,000 features).\n",
        "4. **Models Trained**: Naive Bayes, SVM, Logistic Regression, Random Forest.\n",
        "5. **Evaluation**: All models achieved high performance; metrics include Accuracy, Precision, Recall, F1-Score.\n",
        "6. **Cross-Validation**: 5-fold CV confirmed robust generalization.\n",
        "7. **Hyperparameter Tuning**: GridSearchCV optimized the best model further.\n",
        "8. **Model Saved**: Best model and vectorizer exported for deployment in the web application.\n",
        "\n",
        "### **Next Steps (Assignment 3 \\u2014 Web Application):**\n",
        "- Integrate the saved model into a **FastAPI** backend\n",
        "- Build a **React/HTML** frontend for user interaction\n",
        "- Add data visualization features to help users understand results\n",
        "- Deploy the application for user testing\n",
        "\n",
        "---\n",
        "*TechNova Team COS30049 Computing Technology Innovation Project*\n"
      ],
      "metadata": {
        "id": "-SW9pNTQ-J3v"
      }
    }
  ]
}